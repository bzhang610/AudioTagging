{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "from collections import Counter, defaultdict\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from psutil import cpu_count\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imgaug import augmenters as iaa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 80\n",
    "SIZE=128\n",
    "checkpoint_file = ['model_best1.h5', 'model_best2.h5', 'model_best3.h5']\n",
    "# See Version40 for 3 snapshots (or you can use only 1 which is normal run)\n",
    "EPOCHS = [432, 0, 0] #150 for inception, 100 for xception\n",
    "TTA = [19, 0, 0] #Number of test-time augmentation\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "LR = 4e-4\n",
    "PATIENCE = 10 #ReduceOnPlateau option\n",
    "LR_FACTOR = 0.8 #ReduceOnPlateau option\n",
    "CURATED_ONLY = True # use only curated data for training\n",
    "TRAIN_AUGMENT = True # use augmentation for training data?\n",
    "VALID_AUGMENT = False\n",
    "MODEL = 'mobile' #'cnn8th' # choose among 'xception', 'inception', 'mobile', 'crnn', 'simple'\n",
    "SEED = 520\n",
    "\n",
    "USE_MIXUP = True\n",
    "MIXUP_PROB = 0.275\n",
    "\n",
    "# No K-Fold implementation yet\n",
    "# NUM_K_FOLDS = 5 # how many folds (K) you gonna splits\n",
    "# NUM_MODEL_RUN = 5 # how many models (<= K) you gonna train [e.g. set to 1 for a simple train/test split]\n",
    "\n",
    "# if use BCEwithLogits loss, use Activation = 'linear' only\n",
    "#ACTIVATION = 'linear' \n",
    "ACTIVATION = 'softmax'\n",
    "# ACTIVATION = 'sigmoid'\n",
    "\n",
    "LOSS = 'categorical_crossentropy'\n",
    "# LOSS = 'binary_crossentropy' \n",
    "#LOSS = 'BCEwithLogits' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label-weighted Label Ranking Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# from official code https://colab.research.google.com/drive/1AgPdhSp7ttY18O3fEoHOQKlt_3HJDLi8#scrollTo=cRCaCIb9oguU\n",
    "def _one_sample_positive_class_precisions(scores, truth):\n",
    "    \"\"\"Calculate precisions for each true class for a single sample.\n",
    "\n",
    "    Args:\n",
    "      scores: np.array of (num_classes,) giving the individual classifier scores.\n",
    "      truth: np.array of (num_classes,) bools indicating which classes are true.\n",
    "\n",
    "    Returns:\n",
    "      pos_class_indices: np.array of indices of the true classes for this sample.\n",
    "      pos_class_precisions: np.array of precisions corresponding to each of those\n",
    "        classes.\n",
    "    \"\"\"\n",
    "    num_classes = scores.shape[0]\n",
    "    pos_class_indices = np.flatnonzero(truth > 0)\n",
    "    # Only calculate precisions if there are some true classes.\n",
    "    if not len(pos_class_indices):\n",
    "        return pos_class_indices, np.zeros(0)\n",
    "    # Retrieval list of classes for this sample.\n",
    "    retrieved_classes = np.argsort(scores)[::-1]\n",
    "    # class_rankings[top_scoring_class_index] == 0 etc.\n",
    "    class_rankings = np.zeros(num_classes, dtype=np.int)\n",
    "    class_rankings[retrieved_classes] = range(num_classes)\n",
    "    # Which of these is a true label?\n",
    "    retrieved_class_true = np.zeros(num_classes, dtype=np.bool)\n",
    "    retrieved_class_true[class_rankings[pos_class_indices]] = True\n",
    "    # Num hits for every truncated retrieval list.\n",
    "    retrieved_cumulative_hits = np.cumsum(retrieved_class_true)\n",
    "    # Precision of retrieval list truncated at each hit, in order of pos_labels.\n",
    "    precision_at_hits = (\n",
    "            retrieved_cumulative_hits[class_rankings[pos_class_indices]] /\n",
    "            (1 + class_rankings[pos_class_indices].astype(np.float)))\n",
    "    return pos_class_indices, precision_at_hits\n",
    "\n",
    "\n",
    "def calculate_per_class_lwlrap(truth, scores):\n",
    "    \"\"\"Calculate label-weighted label-ranking average precision.\n",
    "\n",
    "    Arguments:\n",
    "      truth: np.array of (num_samples, num_classes) giving boolean ground-truth\n",
    "        of presence of that class in that sample.\n",
    "      scores: np.array of (num_samples, num_classes) giving the classifier-under-\n",
    "        test's real-valued score for each class for each sample.\n",
    "\n",
    "    Returns:\n",
    "      per_class_lwlrap: np.array of (num_classes,) giving the lwlrap for each\n",
    "        class.\n",
    "      weight_per_class: np.array of (num_classes,) giving the prior of each\n",
    "        class within the truth labels.  Then the overall unbalanced lwlrap is\n",
    "        simply np.sum(per_class_lwlrap * weight_per_class)\n",
    "    \"\"\"\n",
    "    assert truth.shape == scores.shape\n",
    "    num_samples, num_classes = scores.shape\n",
    "    # Space to store a distinct precision value for each class on each sample.\n",
    "    # Only the classes that are true for each sample will be filled in.\n",
    "    precisions_for_samples_by_classes = np.zeros((num_samples, num_classes))\n",
    "    for sample_num in range(num_samples):\n",
    "        pos_class_indices, precision_at_hits = (\n",
    "            _one_sample_positive_class_precisions(scores[sample_num, :],\n",
    "                                                  truth[sample_num, :]))\n",
    "        precisions_for_samples_by_classes[sample_num, pos_class_indices] = (\n",
    "            precision_at_hits)\n",
    "    labels_per_class = np.sum(truth > 0, axis=0)\n",
    "    weight_per_class = labels_per_class / float(np.sum(labels_per_class))\n",
    "    # Form average of each column, i.e. all the precisions assigned to labels in\n",
    "    # a particular class.\n",
    "    per_class_lwlrap = (np.sum(precisions_for_samples_by_classes, axis=0) /\n",
    "                        np.maximum(1, labels_per_class))\n",
    "    # overall_lwlrap = simple average of all the actual per-class, per-sample precisions\n",
    "    #                = np.sum(precisions_for_samples_by_classes) / np.sum(precisions_for_samples_by_classes > 0)\n",
    "    #           also = weighted mean of per-class lwlraps, weighted by class label prior across samples\n",
    "    #                = np.sum(per_class_lwlrap * weight_per_class)\n",
    "    return per_class_lwlrap, weight_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# from https://www.kaggle.com/rio114/keras-cnn-with-lwlrap-evaluation/\n",
    "def tf_one_sample_positive_class_precisions(y_true, y_pred) :\n",
    "    num_samples, num_classes = y_pred.shape\n",
    "    \n",
    "    # find true labels\n",
    "    pos_class_indices = tf.where(y_true > 0) \n",
    "    \n",
    "    # put rank on each element\n",
    "    retrieved_classes = tf.nn.top_k(y_pred, k=num_classes).indices\n",
    "    sample_range = tf.zeros(shape=tf.shape(tf.transpose(y_pred)), dtype=tf.int32)\n",
    "    sample_range = tf.add(sample_range, tf.range(tf.shape(y_pred)[0], delta=1))\n",
    "    sample_range = tf.transpose(sample_range)\n",
    "    sample_range = tf.reshape(sample_range, (-1,num_classes*tf.shape(y_pred)[0]))\n",
    "    retrieved_classes = tf.reshape(retrieved_classes, (-1,num_classes*tf.shape(y_pred)[0]))\n",
    "    retrieved_class_map = tf.concat((sample_range, retrieved_classes), axis=0)\n",
    "    retrieved_class_map = tf.transpose(retrieved_class_map)\n",
    "    retrieved_class_map = tf.reshape(retrieved_class_map, (tf.shape(y_pred)[0], num_classes, 2))\n",
    "    \n",
    "    class_range = tf.zeros(shape=tf.shape(y_pred), dtype=tf.int32)\n",
    "    class_range = tf.add(class_range, tf.range(num_classes, delta=1))\n",
    "    \n",
    "    class_rankings = tf.scatter_nd(retrieved_class_map,\n",
    "                                          class_range,\n",
    "                                          tf.shape(y_pred))\n",
    "    \n",
    "    #pick_up ranks\n",
    "    num_correct_until_correct = tf.gather_nd(class_rankings, pos_class_indices)\n",
    "\n",
    "    # add one for division for \"presicion_at_hits\"\n",
    "    num_correct_until_correct_one = tf.add(num_correct_until_correct, 1) \n",
    "    num_correct_until_correct_one = tf.cast(num_correct_until_correct_one, tf.float32)\n",
    "    \n",
    "    # generate tensor [num_sample, predict_rank], \n",
    "    # top-N predicted elements have flag, N is the number of positive for each sample.\n",
    "    sample_label = pos_class_indices[:, 0]   \n",
    "    sample_label = tf.reshape(sample_label, (-1, 1))\n",
    "    sample_label = tf.cast(sample_label, tf.int32)\n",
    "    \n",
    "    num_correct_until_correct = tf.reshape(num_correct_until_correct, (-1, 1))\n",
    "    retrieved_class_true_position = tf.concat((sample_label, \n",
    "                                               num_correct_until_correct), axis=1)\n",
    "    retrieved_pos = tf.ones(shape=tf.shape(retrieved_class_true_position)[0], dtype=tf.int32)\n",
    "    retrieved_class_true = tf.scatter_nd(retrieved_class_true_position, \n",
    "                                         retrieved_pos, \n",
    "                                         tf.shape(y_pred))\n",
    "    # cumulate predict_rank\n",
    "    retrieved_cumulative_hits = tf.cumsum(retrieved_class_true, axis=1)\n",
    "\n",
    "    # find positive position\n",
    "    pos_ret_indices = tf.where(retrieved_class_true > 0)\n",
    "\n",
    "    # find cumulative hits\n",
    "    correct_rank = tf.gather_nd(retrieved_cumulative_hits, pos_ret_indices)  \n",
    "    correct_rank = tf.cast(correct_rank, tf.float32)\n",
    "\n",
    "    # compute presicion\n",
    "    precision_at_hits = tf.truediv(correct_rank, num_correct_until_correct_one)\n",
    "\n",
    "    return pos_class_indices, precision_at_hits\n",
    "\n",
    "def tf_lwlrap(y_true, y_pred):\n",
    "    num_samples, num_classes = y_pred.shape\n",
    "    pos_class_indices, precision_at_hits = (tf_one_sample_positive_class_precisions(y_true, y_pred))\n",
    "    pos_flgs = tf.cast(y_true > 0, tf.int32)\n",
    "    labels_per_class = tf.reduce_sum(pos_flgs, axis=0)\n",
    "    weight_per_class = tf.truediv(tf.cast(labels_per_class, tf.float32),\n",
    "                                  tf.cast(tf.reduce_sum(labels_per_class), tf.float32))\n",
    "    sum_precisions_by_classes = tf.zeros(shape=(num_classes), dtype=tf.float32)  \n",
    "    class_label = pos_class_indices[:,1]\n",
    "    sum_precisions_by_classes = tf.unsorted_segment_sum(precision_at_hits,\n",
    "                                                        class_label,\n",
    "                                                       num_classes)\n",
    "    labels_per_class = tf.cast(labels_per_class, tf.float32)\n",
    "    labels_per_class = tf.add(labels_per_class, 1e-7)\n",
    "    per_class_lwlrap = tf.truediv(sum_precisions_by_classes,\n",
    "                                  tf.cast(labels_per_class, tf.float32))\n",
    "    out = tf.cast(tf.tensordot(per_class_lwlrap, weight_per_class, axes=1), dtype=tf.float32)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Crossentropy Loss with Logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as k\n",
    "def BCEwithLogits(y_true, y_pred):\n",
    "    return K.mean(K.binary_crossentropy(y_true, y_pred, from_logits=True), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pkl(filename):\n",
    "    \"\"\"Load pickle object from file.\"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_train_curated = load_pkl('./Data/mels_train_curated.pkl')\n",
    "mel_train_noisy = load_pkl('./Data/mels_train_noisy.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train labels\n",
    "train_curated = pd.read_csv('train_curated.csv')\n",
    "train_noisy = pd.read_csv('train_noisy.csv')\n",
    "CURATED_ONLY = True\n",
    "if CURATED_ONLY:\n",
    "    train_df = train_curated\n",
    "else:\n",
    "    train_df = pd.concat([train_curated, train_noisy], sort=True, ignore_index=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test labels\n",
    "test_df = pd.read_csv('sample_submission.csv')\n",
    "labels = test_df.columns[1:].tolist()\n",
    "num_classes = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match training to testing labels\n",
    "y_train = np.zeros((len(train_df), num_classes)).astype(int)\n",
    "for i, row in enumerate(train_df['labels'].str.split(',')):\n",
    "    for label in row:\n",
    "        idx = labels.index(label)\n",
    "        y_train[i, idx] = 1\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using only curated data\n",
    "x_train = mel_train_curated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = test_df.columns[1:].tolist()\n",
    "labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(labels)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.zeros((len(train_df), num_classes)).astype(int)\n",
    "for i, row in enumerate(train_df['labels'].str.split(',')):\n",
    "    for label in row:\n",
    "        idx = labels.index(label)\n",
    "        y_train[i, idx] = 1\n",
    "\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras import metrics\n",
    "from keras.optimizers import Adam \n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.inception_v3 import preprocess_input as preprocess_inception\n",
    "from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from keras.applications.mobilenet_v2 import preprocess_input as preprocess_mobile\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.xception import preprocess_input as preprocess_xception\n",
    "\n",
    "from keras.utils import Sequence\n",
    "from sklearn.utils import shuffle\n",
    "def create_model_inception(n_out=NUM_CLASSES):\n",
    "\n",
    "    base_model =InceptionV3(weights=None, include_top=False)\n",
    "    \n",
    "    x0 = base_model.output\n",
    "    x1 = GlobalAveragePooling2D()(x0)\n",
    "    x2 = GlobalMaxPooling2D()(x0)\n",
    "    x = Concatenate()([x1,x2])\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    \n",
    "    predictions = Dense(n_out, activation=ACTIVATION)(x)\n",
    "\n",
    "    # this is the model we will train\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_xception(n_out=NUM_CLASSES):\n",
    "\n",
    "    base_model = Xception(weights=None, include_top=False)\n",
    "    \n",
    "    x0 = base_model.output\n",
    "    x1 = GlobalAveragePooling2D()(x0)\n",
    "    x2 = GlobalMaxPooling2D()(x0)\n",
    "    x = Concatenate()([x1,x2])\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "#     x = Dense(128, activation='relu')(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Dropout(0.3)(x)\n",
    "    \n",
    "    predictions = Dense(n_out, activation=ACTIVATION)(x)\n",
    "\n",
    "    # this is the model we will train\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.applications.inception_resnet_v2 import preprocess_input as preprocess_inceptionRes\n",
    "def create_model_inceptionRes(n_out=NUM_CLASSES):\n",
    "\n",
    "    base_model = InceptionResNetV2(weights='imagenet', include_top=False)\n",
    "    \n",
    "    x0 = base_model.output\n",
    "    x1 = GlobalAveragePooling2D()(x0)\n",
    "    x2 = GlobalMaxPooling2D()(x0)\n",
    "    x = Concatenate()([x1,x2])\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "#     x = Dense(128, activation='relu')(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Dropout(0.3)(x)\n",
    "    \n",
    "    predictions = Dense(n_out, activation=ACTIVATION)(x)\n",
    "\n",
    "    # this is the model we will train\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "def create_model_mobile(n_out=NUM_CLASSES):\n",
    "\n",
    "    base_model =MobileNetV2(weights=None, include_top=False)\n",
    "    \n",
    "    x0 = base_model.output\n",
    "    x1 = GlobalAveragePooling2D()(x0)\n",
    "    x2 = GlobalMaxPooling2D()(x0)\n",
    "    x = Concatenate()([x1,x2])\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "#     x = Dense(128, activation='relu')(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Dropout(0.25)(x)\n",
    "\n",
    "    \n",
    "    predictions = Dense(n_out, activation=ACTIVATION)(x)\n",
    "\n",
    "    # this is the model we will train\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_simple_block(x, n_filters):\n",
    "    \n",
    "    x = Convolution2D(n_filters, (3,1), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    \n",
    "    x = Convolution2D(n_filters, (3,1), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = AveragePooling2D()(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def create_model_simplecnn(n_out=NUM_CLASSES):\n",
    "    \n",
    "    inp = Input(shape=(128,128,3))\n",
    "#     inp = Input(shape=(None,None,3))\n",
    "    x = conv_simple_block(inp,64)\n",
    "    x = conv_simple_block(x,128)\n",
    "    x = conv_simple_block(x,256)\n",
    "    x = conv_simple_block(x,128)\n",
    "    \n",
    "#     x1 = GlobalAveragePooling2D()(x)\n",
    "#     x2 = GlobalMaxPooling2D()(x)\n",
    "#     x = Add()([x1,x2])\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    x = Dense(128, activation='linear')(x)\n",
    "    x = PReLU()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    predictions = Dense(n_out, activation=ACTIVATION)(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=predictions)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_of_lambda(input_shape):\n",
    "    return (input_shape[0], input_shape[2], input_shape[3])\n",
    "\n",
    "def my_max(x):\n",
    "    return K.max(x, axis=1, keepdims=False)\n",
    "\n",
    "def crnn_simple_block(x, n_filters):\n",
    "    \n",
    "    x = Convolution2D(n_filters, (3,1), padding=\"same\")(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    \n",
    "    x = Convolution2D(n_filters, (3,1), padding=\"same\")(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = MaxPooling2D()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def create_model_crnn(n_out=NUM_CLASSES):\n",
    "    \n",
    "#     inp = Input(shape=(128,128,3))\n",
    "    inp = Input(shape=(128,None,3))\n",
    "    x = crnn_simple_block(inp,64)\n",
    "    x = crnn_simple_block(x,128)\n",
    "    x = crnn_simple_block(x,256)\n",
    "    \n",
    "    # eliminate the frequency dimension, x = (batch, time, channels)\n",
    "    x = Lambda(my_max, output_shape=output_of_lambda)(x)\n",
    "    \n",
    "    x = Bidirectional(CuDNNGRU(128, return_sequences=True))(x)\n",
    "#     x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    x = Dense(128, activation='linear')(x)\n",
    "    x = PReLU()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    predictions = Dense(n_out, activation=ACTIVATION)(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=predictions)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the 8th solution in 2018 competition\n",
    "# https://github.com/sainathadapa/kaggle-freesound-audio-tagging\n",
    "def create_model_cnn8th(n_out=NUM_CLASSES):\n",
    "    regu=0\n",
    "    inp = Input(shape=(128,128,3))\n",
    "\n",
    "    x = Conv2D(48, 11,  strides=(1,1),kernel_initializer='he_uniform', activation='relu', padding='same',kernel_regularizer=regularizers.l2(regu))(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(48, 11,  strides=(2,3),kernel_initializer='he_uniform', activation='relu', padding='same',kernel_regularizer=regularizers.l2(regu))(x)\n",
    "    x = MaxPooling2D(3, strides=(1,2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Conv2D(128, 5, strides=(1,1),kernel_initializer='he_uniform', activation='relu', padding='same',kernel_regularizer=regularizers.l2(regu))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(128, 5, strides=(2,3),kernel_initializer='he_uniform', activation='relu', padding='same',kernel_regularizer=regularizers.l2(regu))(x)\n",
    "    x = MaxPooling2D(3, strides=2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Conv2D(192, 3, strides=1,kernel_initializer='he_uniform', activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(192, 3, strides=1,kernel_initializer='he_uniform', activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(128, 3, strides=1,kernel_initializer='he_uniform', activation='relu', padding='same',kernel_regularizer=regularizers.l2(regu))(x)\n",
    "    x = MaxPooling2D(3, strides=(1,2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    predictions = Dense(n_out, activation=ACTIVATION)(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=predictions)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.vgg19 import preprocess_input as preprocess_vgg\n",
    "def create_model_vgg(n_out=NUM_CLASSES):\n",
    "\n",
    "    base_model = VGG19(weights='imagenet', include_top=False,input_shape = (SIZE,SIZE, 3))\n",
    "    for layer in base_model.layers[:-4]:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    #x0 = base_model.output\n",
    "    \n",
    "    \n",
    "    #x1 = GlobalAveragePooling2D()(x0)\n",
    "    #x2 = GlobalMaxPooling2D()(x0)\n",
    "    #x = Concatenate()([x1,x2])\n",
    "    \n",
    "    #x = BatchNormalization()(x)\n",
    "    #x = Dropout(0.5)(x)\n",
    "    \n",
    "    #x = Dense(256, activation='relu')(x)\n",
    "    #x = BatchNormalization()(x)\n",
    "    #x = Dropout(0.5)(x)\n",
    "\n",
    "    #Adding custom Layers \n",
    "    x = base_model.output\n",
    "    x = BatchNormalization()(x)\n",
    "    #x = Dropout(0.5)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1024, activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1024, activation=\"relu\")(x)\n",
    "    \n",
    "    predictions = Dense(n_out, activation=ACTIVATION)(x)\n",
    "\n",
    "    # this is the model we will train\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_input = preprocess_vgg\n",
    "model = create_model_vgg(n_out=NUM_CLASSES)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "'''Choose your model here'''\n",
    "if MODEL == 'xception':\n",
    "    preprocess_input = preprocess_xception\n",
    "    model = create_model_xception(n_out=NUM_CLASSES)\n",
    "elif MODEL == 'inception':\n",
    "    preprocess_input = preprocess_inception\n",
    "    model = create_model_inception(n_out=NUM_CLASSES)\n",
    "elif MODEL == 'mobile':\n",
    "    preprocess_input = preprocess_mobile\n",
    "    model = create_model_mobile(n_out=NUM_CLASSES)\n",
    "elif MODEL == 'crnn':\n",
    "    preprocess_input = preprocess_mobile\n",
    "    model = create_model_crnn(n_out=NUM_CLASSES)\n",
    "elif MODEL == 'cnn8th':\n",
    "    preprocess_input = preprocess_mobile\n",
    "    model = create_model_cnn8th(n_out=NUM_CLASSES)\n",
    "else:\n",
    "    preprocess_input = preprocess_mobile\n",
    "    model = create_model_simplecnn(n_out=NUM_CLASSES)\n",
    "\n",
    "print(MODEL)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want, you can try more advanced augmentation like this\n",
    "augment_img = iaa.Sequential([\n",
    "#         iaa.ContrastNormalization((0.9, 1.1)),\n",
    "#         iaa.Multiply((0.9, 1.1), per_channel=0.2),\n",
    "        iaa.Fliplr(0.5),\n",
    "#         iaa.GaussianBlur(sigma=(0, 0.1)),\n",
    "#         iaa.Affine( # x-shift\n",
    "#             translate_percent={\"x\": (-0.1, 0.1), \"y\": (-0.0, 0.0)},\n",
    "#         ),\n",
    "        iaa.CoarseDropout(0.12,size_percent=0.05) # see examples : https://github.com/aleju/imgaug\n",
    "            ], random_order=True)\n",
    "\n",
    "\n",
    "\n",
    "# Or you can choose this simplest augmentation (like pytorch version)\n",
    "# augment_img = iaa.Fliplr(0.5)\n",
    "\n",
    "# This is my ugly modification; sorry about that\n",
    "class FATTrainDataset(Sequence):\n",
    "\n",
    "    def mix_up(x, y):\n",
    "        x = np.array(x, np.float32)\n",
    "        lam = np.random.beta(1.0, 1.0)\n",
    "        ori_index = np.arange(int(len(x)))\n",
    "        index_array = np.arange(int(len(x)))\n",
    "        np.random.shuffle(index_array)        \n",
    "        \n",
    "        mixed_x = lam * x[ori_index] + (1 - lam) * x[index_array]\n",
    "        mixed_y = lam * y[ori_index] + (1 - lam) * y[index_array]\n",
    "        \n",
    "        return mixed_x, mixed_y\n",
    "    \n",
    "    def getitem(image):\n",
    "        # crop 2sec\n",
    "\n",
    "        base_dim, time_dim, _ = image.shape\n",
    "        crop = random.randint(0, time_dim - base_dim)\n",
    "        image = image[:,crop:crop+base_dim,:]\n",
    "\n",
    "        image = preprocess_input(image)\n",
    "        \n",
    "#         label = self.labels[idx]\n",
    "        return image\n",
    "    def create_generator(train_X, train_y, batch_size, shape, augument=False, shuffling=False, test_data=False, mixup=False, mixup_prob=0.3):\n",
    "        assert shape[2] == 3\n",
    "        while True:\n",
    "            if shuffling:\n",
    "                train_X,train_y = shuffle(train_X,train_y)\n",
    "\n",
    "            for start in range(0, len(train_y), batch_size):\n",
    "                end = min(start + batch_size, len(train_y))\n",
    "                batch_images = []\n",
    "                X_train_batch = train_X[start:end]\n",
    "                if test_data == False:\n",
    "                    batch_labels = train_y[start:end]\n",
    "                \n",
    "                for i in range(len(X_train_batch)):\n",
    "                    image = FATTrainDataset.getitem(X_train_batch[i])   \n",
    "                    if augument:\n",
    "                        image = FATTrainDataset.augment(image)\n",
    "                    batch_images.append(image)\n",
    "                \n",
    "                if (mixup and test_data == False):\n",
    "                    dice = np.random.rand(1)\n",
    "                    if dice > mixup_prob:\n",
    "                        batch_images, batch_labels =  FATTrainDataset.mix_up(batch_images, batch_labels)    \n",
    "                    \n",
    "                if test_data == False:\n",
    "                    yield np.array(batch_images, np.float32), batch_labels\n",
    "                else:\n",
    "                    yield np.array(batch_images, np.float32)\n",
    "        return image\n",
    "    \n",
    "    def augment(image):\n",
    "\n",
    "        image_aug = augment_img.augment_image(image)\n",
    "        return image_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import (ModelCheckpoint, LearningRateScheduler,\n",
    "                             EarlyStopping, ReduceLROnPlateau,CSVLogger)\n",
    "                             \n",
    "from sklearn.model_selection import train_test_split,KFold\n",
    "\n",
    "\n",
    "#reduceLROnPlat = ReduceLROnPlateau(monitor='val_tf_lwlrap', factor=LR_FACTOR, patience=PATIENCE, verbose=1, mode='max', min_delta=0.0001, cooldown=2, min_lr=1e-5 )\n",
    "\n",
    "#csv_logger = CSVLogger(filename='../working/training_log.csv',separator=',',append=True)\n",
    "\n",
    "#checkpoint = ModelCheckpoint(checkpoint_file[0], monitor='val_tf_lwlrap', verbose=1, save_best_only=True, mode='max', save_weights_only = False)\n",
    "\n",
    "#callbacks_list = [checkpoint, csv_logger, reduceLROnPlat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train, valid\n",
    "x_trn, x_val, y_trn, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=SEED)\n",
    "\n",
    "# create train and valid datagens\n",
    "train_generator = FATTrainDataset.create_generator(\n",
    "    x_trn, y_trn, BATCH_SIZE, (SIZE,SIZE,3), augument=TRAIN_AUGMENT, shuffling=True, mixup = USE_MIXUP, mixup_prob = MIXUP_PROB)\n",
    "validation_generator = FATTrainDataset.create_generator(\n",
    "    x_val, y_val, BATCH_SIZE, (SIZE,SIZE,3), augument=VALID_AUGMENT, shuffling=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_steps = np.ceil(float(len(x_trn)) / float(BATCH_SIZE))\n",
    "val_steps = np.ceil(float(len(x_val)) / float(BATCH_SIZE))\n",
    "train_steps = train_steps.astype(int)\n",
    "val_steps = val_steps.astype(int)\n",
    "print(train_steps, val_steps)\n",
    "print(len(x_trn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(LOSS)\n",
    "if LOSS=='BCEwithLogits':\n",
    "     model.compile(loss=BCEwithLogits,\n",
    "            optimizer=Adam(lr=LR),\n",
    "            metrics=[tf_lwlrap,'categorical_accuracy'])\n",
    "else:\n",
    "    model.compile(loss=LOSS,\n",
    "            optimizer=Adam(lr=LR),\n",
    "            metrics=[tf_lwlrap,'categorical_accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(LR, PATIENCE, LR_FACTOR,BATCH_SIZE, TRAIN_AUGMENT, USE_MIXUP, MIXUP_PROB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_steps,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=val_steps,\n",
    "    #epochs=EPOCHS[0],\n",
    "    epochs=200,\n",
    "    verbose=1)\n",
    "    #,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(K.eval(model.optimizer.lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if LOSS=='BCEwithLogits':\n",
    "#      model.compile(loss=BCEwithLogits,\n",
    "#             optimizer=Adam(lr=3e-4),\n",
    "#             metrics=[tf_lwlrap,'categorical_accuracy'])\n",
    "# else:\n",
    "#     model.compile(loss=LOSS,\n",
    "#             optimizer=Adam(lr=3e-4),\n",
    "#             metrics=[tf_lwlrap,'categorical_accuracy'])\n",
    "\n",
    "# train_generator = FATTrainDataset.create_generator(\n",
    "#     x_trn, y_trn, BATCH_SIZE, (SIZE,SIZE,3), augument=TRAIN_AUGMENT, \n",
    "#     shuffling=True, mixup = False, mixup_prob=0.1)\n",
    "\n",
    "# EPOCHS = [100, 66, 0]\n",
    "\n",
    "# print(K.eval(model.optimizer.lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
    "ax[0].set_title('loss')\n",
    "ax[0].plot(hist.epoch, hist.history[\"loss\"], label=\"Train loss\")\n",
    "ax[0].plot(hist.epoch, hist.history[\"val_loss\"], label=\"Validation loss\")\n",
    "ax[1].set_title('categorical_accuracy')\n",
    "ax[1].plot(hist.epoch, hist.history[\"categorical_accuracy\"], label=\"Train categorical_accuracy\")\n",
    "ax[1].plot(hist.epoch, hist.history[\"val_categorical_accuracy\"], label=\"Validation categorical_accuracy\")\n",
    "ax[0].legend()\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
    "ax[0].set_title('tf_lwlrap')\n",
    "ax[0].plot(hist.epoch, hist.history[\"tf_lwlrap\"], label=\"Train lwlrap\")\n",
    "ax[0].plot(hist.epoch, hist.history[\"val_tf_lwlrap\"], label=\"Validation lwlrap\")\n",
    "ax[1].set_title('categorical_accuracy')\n",
    "ax[1].plot(hist.epoch, hist.history[\"categorical_accuracy\"], label=\"Train categorical_accuracy\")\n",
    "ax[1].plot(hist.epoch, hist.history[\"val_categorical_accuracy\"], label=\"Validation categorical_accuracy\")\n",
    "ax[0].legend()\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
